import pandas as pd
from datetime import datetime
from sklearn.preprocessing import minmax_scale
from skcriteria import Data, MIN, MAX
from skcriteria.madm import simple

# Expands data visibility in python terminal
pd.options.mode.chained_assignment = None
pd.set_option('display.max_columns', 500)
pd.set_option('display.max_rows', 200)
pd.set_option('display.width', 600)

# Finds percent of missing values, here if you need to run it in  the Python terminal...
def missing_values(dataframe):
    percent_missing = dataframe.isnull().sum() * 100 / len(dataframe)
    missing_values_df = pd.DataFrame({'column_name': dataframe.columns,
                                      'percent_missing': percent_missing})
    missing_values_df.sort_values('percent_missing', ascending=False, inplace=True)
    return missing_values_df

''' If there's a decimal in the column values, round down and convert to integer, else if something needs to be added 
to the values of targeted columns, else if you need to change Int64 type to datetime types... '''

def modify_column_values(dataframe, *column_list, add_thing=None, floater=False, time_change=False):
    if floater:
        # If type FLOAT, round down and change type to STRING
        for na in dataframe[([*column_list])]:
            if len([*column_list]) > 1:
                dataframe = dataframe[dataframe[na].notna()]
                dataframe.loc[:, na] = dataframe[na].astype(int)
                dataframe.loc[:, na] = dataframe[na].astype(str) + add_thing
                dataframe[na] = dataframe[na].apply(lambda _: datetime.strptime(_, '%Y-%m-%d').date().year)

            else:
                dataframe = dataframe[dataframe[na].notna()]
                dataframe.loc[:, na] = dataframe[na].astype(int)
                dataframe.loc[:, na] = dataframe[na].astype(str) + add_thing
                dataframe[na] = dataframe[na].apply(lambda _: datetime.strptime(_, '%Y-%m-%d').date().year)

        return dataframe

    elif time_change:
        for na in dataframe[([*column_list])]:
            dataframe[na] = dataframe[na].astype('datetime64[ns]')
            return dataframe


# To replace locations with more concise nomenclature
def replace_column_values(dataframe, column_name, *new_values):
    for val in new_values:
        dataframe.loc[dataframe[column_name].str.contains(val, na=False), column_name] = val


def reverse_blend(dataframe):
    dframe = dataframe.copy()
    dframe['Blended Coverage'] = dframe['Blended Coverage'].apply(lambda x: (-1 * x + 100) / 100)

    return dframe


''' Normalizing attributes, the closer to one the better the ranking, the closer to zero, the worse the ranking '''
def min_max_scaling(dframe, *columns):
    colz = list(columns)
    df_norm = dframe.copy()

    for col in colz:
        df_norm[col] = (df_norm[col] - df_norm[col].min()) / (df_norm[col].max() - df_norm[col].min())

    return df_norm

# Obtain DataFrame and correct for missing values
df = pd.read_csv(r'C:\Users\cparker\Desktop\Updates\Copy of REID DDG DENSO O2 AFRS 010521.csv')
df['Manual'] = df['Manual'].str.lower().fillna('')
df[['Last 12 Mo Sales', 'Local Experian VIO', 'Local Vista Demand', 'Alliance DW Region']] = df[['Last 12 Mo Sales',
                                                                                                 'Local Experian VIO',
                                                                                                 'Local Vista Demand',
                                                                                                 'Alliance DW Region']].fillna(0)
df = modify_column_values(df, 'Min Model Year', 'Max Model Year', add_thing='-01-01', floater=True)

# Reverse Blended Coverage to reflect same "direction of goodness" as the attributes...
df1 = reverse_blend(df)

# filter by location while Normalizing Attributes (min-max-scaling)
locations = {}
df_inv = {}
for loc in df1['Location'].unique():
    locations[loc] = df1.loc[df1['Location'] == loc]
    df_inv[loc[13:]] = min_max_scaling(locations[loc], 'Last 12 Mo Sales', 'Local Experian VIO', 'Local Vista Demand',
                                  'Alliance DW Region', 'Blended Coverage')

# MCDM for all Locations using WSM Algorithm (Weighted Sum Method) /\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\
for loc in df_inv.keys():
    dis_frame = df_inv[loc][
        ['Part', 'Last 12 Mo Sales', 'Local Experian VIO', 'Local Vista Demand', 'Alliance DW Region', 'Blended Coverage']]

    criteria_data = Data(
        dis_frame.iloc[:, 1:],
        [MAX, MAX, MAX, MAX, MAX],
        anames=dis_frame['Part'],
        cnames=dis_frame.columns[1:],
        weights=[1, 1, 1.25, 1, 1]
    )

    dm = simple.WeightedSum(mnorm="sum")
    dec = dm.decide(criteria_data)

    # Convert Ranks to Series to insert into Dataframe
    parts = pd.Series(dec.data.anames, name="Part")
    ranks = pd.Series(dec.rank_, name="Rank")
    ranked_df = pd.concat([parts, ranks], axis=1)

    # Obtain Original DataFrame for each Location
    og_df = df.loc[df['Location'].str.contains(loc)]

    # Merge rankings onto DataFrame and rearranging columns
    new_df = og_df.merge(ranked_df, how='inner', on='Part')
    cols = new_df.columns.to_list()
    cols = cols[:12] + cols[-1:] + cols[12:-1]
    new_df = new_df[cols]

    # Write to EXCEL file:
    new_df.to_excel(rf'C:\Users\cparker\Desktop\Updates\{loc}_RANKINGS(old script).xlsx', index=False, engine='openpyxl')
